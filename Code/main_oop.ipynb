{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cbcedaa-bc0b-456a-aa81-b05a884a12f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from Features import Feature_Info\n",
    "from Data import IncomingData, Missing_info\n",
    "from Bounds import Bounds\n",
    "from Dual_Annealing_Optimization import Dual_Annealing_Optimization\n",
    "\n",
    "# from Postprocessing import Avoid_small_changes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Protocol\n",
    "import os.path as osp\n",
    "from shutil import move\n",
    "\n",
    "from utils import read_json, read_pickle, write_json\n",
    "from data_mapper import MapperHandler, read_input_folder, put_data\n",
    "import re\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "from validators import NOConnectionError, connection_check\n",
    "from similarity import search_dates\n",
    "\n",
    "from Checker import Np_KPI_checker, Ng_KPI_checker, Original_product_checker, Optimized_product_checker\n",
    "from reporter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3983f7bb-e612-4770-b7ca-a5aea54b4bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatstr = '%(asctime)s: %(levelname)s: %(funcName)s Line: %(lineno)d %(message)s'\n",
    "datestr = '%m/%d/%Y %H:%M:%S'\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format=formatstr, \n",
    "    datefmt=datestr, \n",
    "    handlers=[\n",
    "        logging.FileHandler('data_download.log'),\n",
    "        logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# set up second logger to track missing variables\n",
    "handler = logging.FileHandler('missing.log')\n",
    "handler.setFormatter(logging.Formatter(fmt=formatstr, datefmt=datestr))\n",
    "\n",
    "missing_logger = logging.getLogger(\"missing_logger\")\n",
    "missing_logger.setLevel(logging.WARNING)\n",
    "missing_logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba78df89-f75b-43ab-8e54-d4dc739e1d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zxu0222\\Anaconda3\\envs\\sql-en\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "# read configs any other files for setup\n",
    "config = read_json(\"../Model/info.json\")\n",
    "reporter_info = read_json('../Model/reporter_info.json')\n",
    "final_model = read_pickle('../Model/xgb_pacol2_sasol_main_product_30mins.pkl')\n",
    "optimizer_info = read_json('../Model/p2_optimizer.json')\n",
    "# previous_lims_ids = read_json('../Model/previous_lims.json')\n",
    "# previous_lims_ids = {int(k):v for k,v in previous_lims_ids.items()}\n",
    "\n",
    "# NOTE: load additional configs for the similarity check\n",
    "short_to_long_map = read_json('../Model/short_to_long_name_map.json')\n",
    "\n",
    "\n",
    "# combine the all object ids into a single variable\n",
    "object_ids = [*config['controllable'], *config['noncontrollable'], *config['additional_ids'], *config['lims_ids']]\n",
    "\n",
    "# separator to use ...FIXME: figure out where to put this elsewhere\n",
    "sep = '___' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "143bf48a-8a82-4f58-9bc2-aef11837d5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize reporters\n",
    "# Need to be improved\n",
    "live_connection_checker = LiveConnectionChecker(*reporter_info['LiveConnection'])\n",
    "missing_tag_checker = MissingDataChecker(*reporter_info['MissingTagCount'])\n",
    "new_directive_checker = NewDirectiveChecker(*reporter_info['NewDirective'])\n",
    "predicted_kpi_checker = PredictedKPIChecker(*reporter_info['PredictedKPI'])\n",
    "optimized_kpi_checker = OptimizedKPIChecker(*reporter_info['OptimizedKPI'])\n",
    "np_KPI_checker = NPSpecificConsChecker(*reporter_info['NPSpecificCons'])\n",
    "ng_KPI_checker = NGSpecificConsChecker(*reporter_info['NGSpecificCons'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c784d906-23c6-432b-bea0-773a4f679811",
   "metadata": {},
   "outputs": [],
   "source": [
    "mh = MapperHandler('Datapoint Name', 'Datapoint Id', 'Property Name', 'Property Id')\n",
    "mapper, sub, name_to_id, id_to_name, final_object_ids, final_property_ids = mh.read_filter_create_extract(\n",
    "    mapper_path=config['mapper_path'], object_ids=object_ids, property_ids=config['property_ids'], \n",
    "    *config['read_params']['args'], **config['read_params']['kwargs'])\n",
    "reader = read_input_folder('../Input-30min/', '.csv', object_ids=final_object_ids, property_ids=final_property_ids, id_to_name=id_to_name, sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d953fbb6-5dbf-43f9-a1cd-8168b1462b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved failed files: True\n",
      "starting file: SasolDataExportedAt2023_04_05_18_00.csv\n"
     ]
    }
   ],
   "source": [
    "# preallocate lists to hold incoming files and processed data ...\n",
    "files = []\n",
    "new_data = []\n",
    "no_connection_files = []\n",
    "\n",
    "for incoming_file, processed_data in reader:\n",
    "\n",
    "    resample_data = processed_data.resample('60T', closed='right').mean()\n",
    "    time = re.findall('\\d{4,}_\\d{2,}_\\d{2,}_\\d{2,}_\\d{2,}', incoming_file)\n",
    "    date_time = pd.to_datetime(time, format = \"%Y_%m_%d_%H_%M\")\n",
    "    resample_data.index = date_time\n",
    "\n",
    "    try:\n",
    "        connection_check(resample_data)\n",
    "    except NOConnectionError as e:\n",
    "        print(e)\n",
    "        # NOTE: add functionality to save file out if the data is missing and set the live connection boolean to false\n",
    "        logging.info(f\"Moving file: {incoming_file} to done folder and setting the live connection boolean to false\")\n",
    "        move(osp.join(args.input_dir, incoming_file), osp.join('../Error', incoming_file))\n",
    "        # NOTE: if connection fails then update the checker\n",
    "        epoch = resample_data.index[0].timestamp()\n",
    "        live_connection_checker.add_value(epoch, 0)\n",
    "        no_connection_files.append(resample_data)     \n",
    "\n",
    "    else:\n",
    "        files.append(incoming_file)\n",
    "        new_data.append(resample_data)\n",
    "        epoch = resample_data.index[0].timestamp()\n",
    "        # NOTE: this is technically redundant since the default is 1 ...\n",
    "        live_connection_checker.add_value(epoch, 1)\n",
    "\n",
    "new_data = pd.concat(list(new_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "613cfcab-9016-4062-af1a-56a3eeb538d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/06/2023 09:54:19: INFO: <cell line: 19> Line: 19 Generating Bounds\n",
      "04/06/2023 09:54:19: INFO: <cell line: 22> Line: 22 Running model and optimization\n"
     ]
    }
   ],
   "source": [
    "previous_data_path = '../Model/previous_tag_vals.csv'\n",
    "previous_data = pd.read_csv(previous_data_path, parse_dates=['Date'], index_col='Date')\n",
    "\n",
    "features = Feature_Info(optimizer_info)\n",
    "incoming_data = IncomingData(new_data, features)\n",
    "\n",
    "missing_info = Missing_info(incoming_data)\n",
    "\n",
    "# Do missing operation here.\n",
    "for i in range(incoming_data.value.shape[0]):\n",
    "    timestamp = incoming_data.value.index[i]\n",
    "    missing_info.missing_log(timestamp, missing_tag_checker)\n",
    "\n",
    "nomissing_data = missing_info.missing_filling(previous_data)\n",
    "\n",
    "# Uncomment this before go live\n",
    "# processed_data.to_csv(previous_vals_path, index_label='Date')\n",
    "\n",
    "logging.info('Generating Bounds')\n",
    "bounds = Bounds(nomissing_data).final_bounds()\n",
    "\n",
    "logging.info('Running model and optimization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53c96d29-9c26-4d8c-b1a9-653b380a3657",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizationLowDifferenceError(Exception):\n",
    "    '''\n",
    "    class to return low difference error.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    percent_different: float, % change from the original value\n",
    "    min_changing_rate: float, % minimum change required by IES\n",
    "    tag: corresponding tag name\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, percent_difference, min_changing_rate, feature_name) -> None:\n",
    "        self.percent_difference = percent_difference\n",
    "        self.min_changing_rate = min_changing_rate\n",
    "        self.feature_name = feature_name\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Optimization values for {self.feature_name} within {self.min_changing_rate:.2f}% of original values. Optimization {self.percent_difference:.2f}% different.\"\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Post_process:\n",
    "    optimal_controls_vals: np.ndarray\n",
    "    nomissing_data: IncomingData\n",
    "\n",
    "    def calculate_percentage_changes(self, timestamp: datetime.datetime) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        method to calculate percentage of change between original vals and optimized vals\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        percent_different : \n",
    "            Percent change between optimal and original\n",
    "        \"\"\"\n",
    "        original_controls_vals = self.nomissing_data.get_control_vals(timestamp)\n",
    "        percent_difference = np.abs((self.optimal_controls_vals-original_controls_vals)/original_controls_vals+0.001)\n",
    "\n",
    "        return percent_difference.values.flatten()\n",
    "    \n",
    "    def track_different_controls(self, timestamp: datetime.datetime) -> List[str]:\n",
    "        \"\"\"\n",
    "        method to return differing controllable tags, difference being defined by threshold\n",
    "\n",
    "        Record the small difference features into the logging\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        different_tags : \n",
    "            Different tags, similar being defined by the threshold \n",
    "        \"\"\"\n",
    "        logger = logging.getLogger(__name__)\n",
    "        \n",
    "        different_tags = []\n",
    "        min_rate_change_threshold = np.array([item['min_rate'] for item in self.nomissing_data.features.controllable.values()])\n",
    "        percentage_differences = self.calculate_percentage_changes(timestamp)\n",
    "        \n",
    "        for controllable_index in range(percentage_differences.shape[0]):\n",
    "            try:\n",
    "                if percentage_differences[controllable_index] > min_rate_change_threshold[controllable_index]:\n",
    "                    different_tags.append(list(self.nomissing_data.features.controllable.keys())[controllable_index])\n",
    "                else:\n",
    "                    raise OptimizationLowDifferenceError(percentage_differences[controllable_index]*100, min_rate_change_threshold[controllable_index]*100, list(self.nomissing_data.features.controllable.keys())[controllable_index])\n",
    "            except OptimizationLowDifferenceError as e:\n",
    "                logger.warning(str(e))\n",
    "\n",
    "        return different_tags\n",
    "    \n",
    "    def domain_fix(self, ) -> List[str]:\n",
    "        '''\n",
    "        method to incorporate domain requirement, \n",
    "        \n",
    "        remove a few directives based on some additional kpis.\n",
    "        \n",
    "        return: a condensed different_tags list\n",
    "        '''\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aea5e393-099c-4b47-a2f2-65bd7c796a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Domain_requirements_failure(Exception):\n",
    "    '''\n",
    "    class to return Domain_requirements_failure.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    percent_different: float, % change from the original value\n",
    "    min_changing_rate: float, % minimum change required by IES\n",
    "    tag: corresponding tag name\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, actual_KPI, required_KPI,requirement_number) -> None:\n",
    "        self.actual_KPI = actual_KPI\n",
    "        self.required_KPI = required_KPI\n",
    "        self.requirement_number = requirement_number\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'Domain requirement_{self.requirement_number} need value {self.required_KPI}, the actual is {self.actual_KPI}.'\n",
    "\n",
    "def domain_requirement_I(data: pd.DataFrame, timestamp: datetime.datetime, different_tags: list):\n",
    "    logger = logging.getLogger(__name__)\n",
    "    try:\n",
    "        if ((actual_KPI:= data.at[timestamp, 'T8.PACOL2.6FIC-1402_A Augusta___Value'])<3) & ('T8.PACOL2.6FIC-1402_A Augusta' in different_tags):\n",
    "            different_tags.remove('T8.PACOL2.6FIC-1402_A Augusta')\n",
    "            raise Domain_requirements_failure(actual_KPI,3,'I')\n",
    "    except Domain_requirements_failure as e:\n",
    "        logger.warning(str(e))\n",
    "    return different_tags\n",
    "\n",
    "def domain_requirement_II(data: pd.DataFrame, timestamp: datetime.datetime, different_tags: list):\n",
    "    logger = logging.getLogger(__name__)\n",
    "    try:\n",
    "        if ((actual_KPI:= data.at[timestamp, 'T8.PACOL2.6FIC-1402 Augusta___Value'])<3) & ('T8.PACOL2.6FIC-1402 Augusta' in different_tags):\n",
    "            different_tags.remove('T8.PACOL2.6FIC-1402 Augusta')\n",
    "            raise Domain_requirements_failure(actual_KPI,3,'II')\n",
    "    except Domain_requirements_failure as e:\n",
    "        logger.warning(str(e))\n",
    "    return different_tags\n",
    "\n",
    "# def domain_requirement_III(data: pd.DataFrame, timestamp: datetime.datetime, different_tags: list):\n",
    "#     logger = logging.getLogger(__name__)\n",
    "#     try:\n",
    "#         if ((actual_KPI:= (np.abs(new_data.at[timestamp, 'T8.PACOL2.6TI-400_4 Augusta___Value'] - new_data.at[timestamp, 'T8.PACOL2.6TI-400_6 Augusta___Value'])>0)) & ('T8.PACOL2.6TIC-403 Augusta' in different_tags):\n",
    "#             different_tags.remove('T8.PACOL2.6TIC-403 Augusta')\n",
    "#             raise Domain_requirements_failure(actual_KPI,3,'III')\n",
    "#     except Domain_requirements_failure as e:\n",
    "#         logger.warning(str(e))\n",
    "#     return different_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d917f32b-a3f1-405e-ba2d-c20caa686d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_dif_controls_dict = {}\n",
    "\n",
    "Optimization = Dual_Annealing_Optimization(nomissing_data, final_model)\n",
    "\n",
    "result = [None] * nomissing_data.value.shape[0]\n",
    "for i in range(nomissing_data.value.shape[0]):\n",
    "    timestamp = nomissing_data.value.index[i]\n",
    "    bound = bounds[i]\n",
    "    \n",
    "    optimal_controls_vals = Optimization.run_optimization(timestamp, bound)\n",
    "    \n",
    "    filter_small_changes = Post_process(optimal_controls_vals, nomissing_data)\n",
    "    different_tags = filter_small_changes.track_different_controls()\n",
    "    \n",
    "    result[i] = optimal_controls_vals\n",
    "    # Add required KPI values\n",
    "\n",
    "    # Optional: add domain requirements and operations\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8ba2736-be1c-4d21-bd2a-e927184d2500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9168371247665071\n",
      "465.49976630955126\n",
      "8.358605\n",
      "8.420508\n"
     ]
    }
   ],
   "source": [
    "checkers = [Np_KPI_checker(nomissing_data.value, timestamp, np_KPI_checker), \n",
    "            Ng_KPI_checker(nomissing_data.value, timestamp, ng_KPI_checker),\n",
    "            Original_product_checker(nomissing_data.value, timestamp, predicted_kpi_checker, Optimization),\n",
    "            Optimized_product_checker(nomissing_data.value, timestamp, predicted_kpi_checker, Optimization, optimal_controls_vals)\n",
    "           ]\n",
    "\n",
    "for checker in checkers:\n",
    "    print(checker.get_checker_value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fc4ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now final results need to be written out:\n",
    "# the final variable should be a dataframe with the timestamp as the index and the objectName{sep}PropertyName as the columns\n",
    "# the contents of the files saved will only be the directives --- not any kpi values or byproduct values ...\n",
    "controllable_vars = list(optimizer_info['controllable'].keys())\n",
    "controllable_vars = [control + sep + \"Value\" for control in controllable_vars]\n",
    "\n",
    "final: pd.DataFrame = pd.DataFrame(result, columns=controllable_vars, index=new_data.index) # this should be the end result of running the optimization\n",
    "\n",
    "historical_data = pd.read_csv('../Model/final_historical_data.csv', parse_dates=['Date']).set_index('Date')\n",
    "historical_data = historical_data.rename(columns=short_to_long_map)\n",
    "historical_data.columns = [col + '___Value' for col in historical_data.columns.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f67c292",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_datapoints = {}\n",
    "for i, timestamp in enumerate(final.index):\n",
    "    # TODO: swap out the controllable_vars with the more general tags_of_interest\n",
    "    this_match  = search_dates(historical_data.loc[:, controllable_vars], final.loc[timestamp, controllable_vars])\n",
    "    similar_datapoints[timestamp] = this_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29edfc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_reporter = AdditionalInfoReporter([live_connection_checker, missing_tag_checker, new_directive_checker, np_specific_cons_checker, ng_specific_cons_checker, predicted_kpi_checker, optimized_kpi_checker])\n",
    "out_prefix = 'data_pacol2_'\n",
    "logging.info('Saving results')\n",
    "for i, timestamp in enumerate(final.index):\n",
    "\n",
    "    ## Note: Error lies in here!!!\n",
    "    live_format, columns, file_name, this_epoch = put_data(final.loc[[timestamp], time_dif_tags_dict[timestamp]], args.output_dir, name_to_id, sep=sep, output_property_id=config['output_property_id'])\n",
    "    # get all the additional info \n",
    "    additional_info = info_reporter.report(epoch=this_epoch)\n",
    "    # append it to the ds info\n",
    "    live_format.extend(additional_info)\n",
    "\n",
    "    # Find validation point \n",
    "    matches = similar_datapoints[timestamp]\n",
    "    if matches.shape[0] != 0 and new_directive_checker.value[timestamp.timestamp()] != 0:\n",
    "        matches.rename(index={matches.index[0]: timestamp},inplace=True)\n",
    "        val_live_format, _, _, _ = put_data(matches, args.output_dir, name_to_id_val, sep=sep, output_property_id=53)\n",
    "        live_format.extend(val_live_format)\n",
    "    \n",
    "    # format to df and save\n",
    "    out = pd.DataFrame(live_format, columns=columns)\n",
    "    file_path = osp.join(args.output_dir, out_prefix+file_name)\n",
    "    out.to_csv(file_path, sep=';', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcce93d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check args"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
